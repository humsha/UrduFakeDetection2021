{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8badf589",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "executionInfo": {
     "elapsed": 814,
     "status": "error",
     "timestamp": 1632947281604,
     "user": {
      "displayName": "Muhammad Humayoun",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhJKAovQAiHD3ElB9IsRGsMRwlp7wTr6TdWi_XTxA=s64",
      "userId": "10868524905129495979"
     },
     "user_tz": -240
    },
    "id": "8badf589",
    "outputId": "69f3deb5-ad8e-453c-abcc-8303a429c4c6"
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import os\n",
    "import pandas \n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n",
    "import matplotlib\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from numpy import array\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import os\n",
    "import pandas \n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n",
    "import matplotlib\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "#All the libraries that we need\n",
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "from numpy import array\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "import string\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import os\n",
    "import pandas \n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import time\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import gensim\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "\n",
    "#Common functions\n",
    "punc=['…','،','۔','،','۔','؟' , '.', ',']\n",
    "NFC = [('آ', 'آ'), ('ة', 'ۃ'), ('ـ', ''), ('ك', 'ک'), ('ه', 'ہ'), ('ۀ', 'ۂ'), \n",
    "       ('ى', 'ی'), ('ي', 'ی'), ('٠', '۰'), ('١', '۱'), ('٢', '۲'), \n",
    "       ('٣', '۳'), ('٤', '۴'), ('٥', '۵'), ('٦', '۶'), ('٧', '۷'), ('٨', '۸'), \n",
    "       ('٩', '۹'), ('ۓ', 'ئے'), ('ۓ', 'ئے'), ('ئ', 'ئی'),\n",
    "       (\"ِ\", \"\"), (\"ِ\", \"\"),(\"ُ\", \"\"),(\"َ\", \"\"),(\"ْ\", \"\"),(\"ٰ\", \"\"),(\"ً\", \"\"),(\"ّ\", \"\")      \n",
    "      ]\n",
    "\n",
    "# load file into memory\n",
    "def load_file(filename):\n",
    "    file = open(filename, 'r', encoding='utf8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def read_as_dict_for_lemma(file):\n",
    "    text = load_file(file)\n",
    "    lines = text.split(\"\\n\")\n",
    "    wd = dict()\n",
    "    for line in lines:\n",
    "        toks = line.split(\":\")\n",
    "        if len(toks)==0 or len(toks)==1:\n",
    "            continue\n",
    "        wd[toks[0].strip()] = toks[1].strip()\n",
    "    return wd\n",
    "\n",
    "#Loading Lemmatizer\n",
    "dictionary = read_as_dict_for_lemma('Humayoun_lemmatizer.txt')\n",
    "\n",
    "def lemma(word, dictionary):\n",
    "    if word in dictionary:\n",
    "        return dictionary[word]\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def read_stopwords(file):\n",
    "    text = load_file(file)\n",
    "    tokens = text.split()\n",
    "    tokens = set(tokens)\n",
    "    return tokens\n",
    "\n",
    "def mkNFC(token):\n",
    "    for (ch1,ch2) in NFC:\n",
    "        token.replace(ch1,ch2)\n",
    "    return token\n",
    "\n",
    "# turn a txt into clean tokens\n",
    "def mk_tokens_vocab(txt, stw=None, lma=None, ngram=1, how=\"all\", ch_ngram=2, ch_how=None):\n",
    "    # split into tokens by white space\n",
    "    tokens = txt.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation+\"\".join(punc))\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [mkNFC(token) for token in tokens]\n",
    "    if stw!=None:\n",
    "        # filter out stop words\n",
    "        stop_words = set(read_stopwords('stopword_Urmono.txt'))\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # filter out short tokens\n",
    "        tokens = [word for word in tokens if len(word) > 1]\n",
    "        tokens = [word for word in tokens if word!='کرنا']\n",
    "        tokens = [word for word in tokens if word!='پرنا']\n",
    "        tokens = [word for word in tokens if word!='رہنا']\n",
    "        tokens = [word for word in tokens if word!='جانا']\n",
    "        tokens = [word for word in tokens if word!='کَیا']\n",
    "    if lma!= None:\n",
    "        #apply lemmatization\n",
    "        tokens = [lemma(token, dictionary) for token in tokens]\n",
    "    \n",
    "    \n",
    "    #getting the char ngrams\n",
    "    charngrams=[]\n",
    "    if ch_how!=None:\n",
    "        charngrams = mkCNgram(tokens, ch_ngram, ch_how)\n",
    "    \n",
    "    wordngrams = mkNgram(tokens, ngram, how)\n",
    "    tokens = charngrams + wordngrams\n",
    "    \n",
    "    #print(tokens)\n",
    "    return tokens\n",
    "\n",
    "# turn a txt into clean tokens\n",
    "def mk_tokens(txt, vocab, stw=None, lma=None, ngram=1, how=\"all\", ch_ngram=2, ch_how=None):\n",
    "    tokens = mk_tokens_vocab(txt, stw, lma, ngram, how, ch_ngram, ch_how)\n",
    "    tokens = [t for t in tokens if t in vocab]\n",
    "    return tokens\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab, stw=None, lma=None, \n",
    "                     ngram=1, how=\"all\", ch_ngram=2, ch_how=None):\n",
    "    # load file\n",
    "    text = load_file(filename)\n",
    "    # clean doc\n",
    "    tokens = mk_tokens_vocab(text, stw, lma, ngram, how, ch_ngram, ch_how)\n",
    "    vocab.update(tokens)\n",
    "\n",
    "\n",
    "def extract_text_label_from_dict(table):\n",
    "    txts=list()\n",
    "    labels=list()\n",
    "    for item in table:\n",
    "        (txt, clabel) = table[item]\n",
    "        txts.append(txt)\n",
    "        labels.append(clabel)\n",
    "    return txts, labels\n",
    "\n",
    "def extract_ids_from_dict(table):\n",
    "    return list(table.keys())\n",
    "\n",
    "def mkNgram(tokens, n=1, how=\"all\"):\n",
    "    if how==\"all\":\n",
    "        grams = list()\n",
    "        for r in range(n):\n",
    "            r+=1\n",
    "            grams.extend([\"_\".join( tokens[i:i+r] ) for i in range(len(tokens)-r+1)]) \n",
    "        return grams\n",
    "    else:\n",
    "        return [\"_\".join( tokens[i:i+n] ) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "\n",
    "#char ngrams\n",
    "def mkCNgram(tokens, L, cn_how=\"all\"):\n",
    "    tokens = \"\".join( tokens[:] )\n",
    "    tokens = mkNgram(tokens, L, cn_how)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_vocab_docs(directory, vocab, stw=None, lma=None, \n",
    "                     ngram=1, how=\"all\", ch_ngram=2, ch_how=None):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab, stw, lma, \n",
    "                         ngram, how, ch_ngram, ch_how)\n",
    "        \n",
    "#=load_file('Urdu-Fake-news-detection-FIRE2021-main/Train/Train/Fake/bs1.txt')       \n",
    "#t\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, \n",
    "                     stw=None, lma=None, \n",
    "                     ngram=1, how=\"all\", \n",
    "                     ch_ngram=2, ch_how=None):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_file(path)\n",
    "        # clean doc\n",
    "        tokens = mk_tokens(doc, vocab, \n",
    "                           stw, lma, \n",
    "                           ngram, how, \n",
    "                           ch_ngram, ch_how)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs_test(directory, vocab, \n",
    "                     stw=None, lma=None, \n",
    "                     ngram=1, how=\"all\", \n",
    "                     ch_ngram=2, ch_how=None):\n",
    "    documents = list()\n",
    "    files=list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_file(path)\n",
    "        # clean doc\n",
    "        tokens = mk_tokens(doc, vocab, \n",
    "                           stw, lma, \n",
    "                           ngram, how, \n",
    "                           ch_ngram, ch_how)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "        files.append(filename.split(\".\")[0])\n",
    "    return (documents, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affc9717",
   "metadata": {
    "id": "affc9717"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cda9abb1",
   "metadata": {
    "id": "cda9abb1"
   },
   "source": [
    "# Deep Convolutional Neural Network for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fd17a9",
   "metadata": {
    "id": "e5fd17a9",
    "outputId": "9e9f44ee-952c-4011-a7a7-a1f95e2e0d89",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "import numpy\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score,accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines, v_char_level):\n",
    "    tokenizer = Tokenizer(char_level=v_char_level)\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "    return max([len(s) for s in lines])\n",
    "\n",
    "\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    padded = pad_sequences(encoded, maxlen=length,  padding='post')\n",
    "    return padded\n",
    "\n",
    "# define the model\n",
    "def define_model(length, vocab_size):\n",
    "    # channel 1\n",
    "    inputs1 = Input(shape=(length,))\n",
    "    embedding1 = Embedding(vocab_size, 100)(inputs1)\n",
    "    conv1 = Conv1D(32, 1, activation='relu')(embedding1)\n",
    "    drop1 = Dropout(0.5)(conv1)\n",
    "    pool1 = MaxPooling1D()(drop1)\n",
    "    flat1 = Flatten()(pool1)\n",
    "    # channel 2\n",
    "    inputs2 = Input(shape=(length,))\n",
    "    embedding2 = Embedding(vocab_size, 100)(inputs2)\n",
    "    conv2 = Conv1D(32, 2, activation='relu')(embedding2)\n",
    "    drop2 = Dropout(0.5)(conv2)\n",
    "    pool2 = MaxPooling1D()(drop2)\n",
    "    flat2 = Flatten()(pool2)\n",
    "    # channel 3\n",
    "    inputs3 = Input(shape=(length,))\n",
    "    embedding3 = Embedding(vocab_size, 100)(inputs3)\n",
    "    conv3 = Conv1D(32, 3, activation='relu')(embedding3)\n",
    "    drop3 = Dropout(0.5)(conv3)\n",
    "    pool3 = MaxPooling1D()(drop3)\n",
    "    flat3 = Flatten()(pool3)\n",
    "    # channel 4\n",
    "    inputs4 = Input(shape=(length,))\n",
    "    embedding4 = Embedding(vocab_size, 100)(inputs4)\n",
    "    conv4 = Conv1D(32, 4, activation='relu')(embedding4)\n",
    "    drop4 = Dropout(0.5)(conv4)\n",
    "    pool4 = MaxPooling1D()(drop4)\n",
    "    flat4 = Flatten()(pool4)\n",
    "    # channel 5\n",
    "    inputs5 = Input(shape=(length,))\n",
    "    embedding5 = Embedding(vocab_size, 100)(inputs5)\n",
    "    conv5 = Conv1D(32, 5, activation='relu')(embedding5)\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    pool5 = MaxPooling1D()(drop5)\n",
    "    flat5 = Flatten()(pool5)\n",
    "    # channel 6\n",
    "    inputs6 = Input(shape=(length,))\n",
    "    embedding6 = Embedding(vocab_size, 100)(inputs6)\n",
    "    conv6 = Conv1D(32, 6, activation='relu')(embedding6)\n",
    "    drop6 = Dropout(0.5)(conv6)\n",
    "    pool6 = MaxPooling1D()(drop6)\n",
    "    flat6 = Flatten()(pool6)\n",
    "    \n",
    "    # merge\n",
    "    merged = concatenate([flat1, flat2, flat3, flat4, flat5, flat6])\n",
    "    # interpretation\n",
    "    dense1 = Dense(10, activation='relu')(merged)\n",
    "    outputs = Dense(1, activation='sigmoid')(dense1)\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3, inputs4, inputs5, inputs6], outputs=outputs)\n",
    "    # compile\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize\n",
    "    #model.summary()\n",
    "    #plot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "    return model\n",
    "\n",
    "def mkExp():\n",
    "    # define vocab  \n",
    "    vocab = Counter()\n",
    "    # add all text to vocab\n",
    "    process_vocab_docs('Urdu-Fake-news-detection-FIRE2021-main/Train/Train/Fake', vocab,\n",
    "                       stw=v_stw, lma=v_lma, \n",
    "                       ngram=v_ngram, how=v_how, \n",
    "                       ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "    process_vocab_docs('Urdu-Fake-news-detection-FIRE2021-main/Train/Train/Real', vocab,\n",
    "                       stw=v_stw, lma=v_lma, \n",
    "                       ngram=v_ngram, how=v_how, \n",
    "                       ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "    # add all text to vocab\n",
    "    process_vocab_docs('Urdu-Fake-news-detection-FIRE2021-main/Train/Test/Fake', vocab,\n",
    "                       stw=v_stw, lma=v_lma, \n",
    "                       ngram=v_ngram, how=v_how, \n",
    "                       ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "    process_vocab_docs('Urdu-Fake-news-detection-FIRE2021-main/Train/Test/Real', vocab,\n",
    "                       stw=v_stw, lma=v_lma, \n",
    "                       ngram=v_ngram, how=v_how, \n",
    "                       ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "\n",
    "\n",
    "    print(\"Most common:\", vocab.most_common(5))\n",
    "    vocab = set(vocab)\n",
    "    print(\"Vocabulary Size:\", len(vocab))\n",
    "\n",
    "\n",
    "    # load all training reviews\n",
    "    fake_docs = process_docs('Urdu-Fake-news-detection-FIRE2021-main/Train/Train/Fake', vocab,\n",
    "                                stw=v_stw, lma=v_lma, \n",
    "                               ngram=v_ngram, how=v_how, \n",
    "                               ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "    real_docs = process_docs('Urdu-Fake-news-detection-FIRE2021-main/Train/Train/Real', vocab,\n",
    "                                stw=v_stw, lma=v_lma, \n",
    "                               ngram=v_ngram, how=v_how, \n",
    "                               ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "    train_docs = real_docs + fake_docs\n",
    "    y = ([0 for _ in range(len(real_docs))] + [1 for _ in range(len(fake_docs))])\n",
    "\n",
    "    #R->0\n",
    "    #F->1\n",
    "    #{0: 750, 1: 550})\n",
    "\n",
    "\n",
    "    # load all test reviews\n",
    "    fake_test_docs = process_docs('Urdu-Fake-news-detection-FIRE2021-main/Train/Test/Fake', vocab,\n",
    "                                stw=v_stw, lma=v_lma, \n",
    "                               ngram=v_ngram, how=v_how, \n",
    "                               ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "    real_test_docs = process_docs('Urdu-Fake-news-detection-FIRE2021-main/Train/Test/Real', vocab,\n",
    "                                stw=v_stw, lma=v_lma, \n",
    "                               ngram=v_ngram, how=v_how, \n",
    "                               ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "    test_docs = real_test_docs + fake_test_docs\n",
    "    ytest = ([0 for _ in range(len(real_test_docs))] + [1 for _ in range(len(fake_test_docs))])\n",
    "\n",
    "    docs = train_docs+test_docs\n",
    "    y = y+ytest\n",
    "    y=array(y)\n",
    "    # summarize the new class distribution\n",
    "    counter = Counter(y)\n",
    "    print(counter)\n",
    "\n",
    "\n",
    "    #Read the test directory   \n",
    "    test_x, files = process_docs_test('Urdu-Fake-news-detection-FIRE2021-main/Test', vocab,\n",
    "                               stw=v_stw, lma=v_lma, \n",
    "                               ngram=v_ngram, how=v_how, \n",
    "                               ch_ngram=v_ch_ngram, ch_how=v_ch_how)\n",
    "\n",
    "    ## create tokenizer\n",
    "    tokenizer = create_tokenizer(docs, v_char_level) \n",
    "    # calculate max document length\n",
    "    length = max_length(docs)\n",
    "    ## load all training reviews\n",
    "    X = encode_text(tokenizer, docs, length)\n",
    "\n",
    "\n",
    "    # calculate max document length\n",
    "    length = max_length(X)\n",
    "    print('Max document length: %d' % length)\n",
    "\n",
    "    # calculate vocabulary size\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    print('Vocabulary size: %d' % vocab_size)\n",
    "    # encode data\n",
    "\n",
    "    # encode training data set\n",
    "    Xtest = encode_text(tokenizer, test_x, length)\n",
    "\n",
    "\n",
    "    # define model\n",
    "    model = define_model(length, vocab_size)\n",
    "    # fit model\n",
    "    model.fit([X,X,X,X,X,X], y, epochs=10, batch_size=7)\n",
    "    print(\"Model trained....\")\n",
    "    \n",
    "    model.save(result_file+'.h5')\n",
    "    # make probability predictions with the model\n",
    "    predictions_scores = model.predict([Xtest,Xtest,Xtest,Xtest,Xtest,Xtest])\n",
    "\n",
    "    test_pred_labels = list()\n",
    "    for score in predictions_scores:\n",
    "        if score>=0.5:\n",
    "            test_pred_labels.append( 1 )\n",
    "        else:\n",
    "            test_pred_labels.append( 0 )\n",
    "\n",
    "    text='File_No,Real/Fake\\n'\n",
    "    for i in range(len(files)):\n",
    "        if test_pred_labels[i]==1:\n",
    "            text+=files[i]+\",F\\n\"\n",
    "        else:\n",
    "            text+=files[i]+\",R\\n\"\n",
    "\n",
    "    afile = open(result_file,\"w\", encoding=\"utf8\")\n",
    "    afile.write(text)\n",
    "    afile.close()\n",
    "    \n",
    "    orig=pd.read_excel('Orignal_labels.xlsx')\n",
    "    #orig.head()\n",
    "    # Added a new column to make labels in Numeric form, like F --> 1, R --> 0\n",
    "    orig['label'] = orig['Real/Fake'].apply(lambda x: 1 if x == 'F' else 0)\n",
    "    \n",
    "    pred=pd.read_csv(result_file)\n",
    "    #pred.head()\n",
    "    \n",
    "    # Added a new column to make labels in Numeric form, like F --> 1, R --> 0\n",
    "    pred['label'] = pred['Real/Fake'].apply(lambda x: 1 if x == 'F' else 0)\n",
    "\n",
    "    pred= pred.sort_values(by='File_No')\n",
    "    cm = confusion_matrix(orig['label'], pred['label'])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    #######################################################\n",
    "    ###################For Fake Class######################\n",
    "\n",
    "    prec_fake = tp/(tp + fp)\n",
    "    print('Precision for Fake class :', prec_fake)\n",
    "\n",
    "    rec_fake = tp/(tp + fn)\n",
    "    print('Recall for Fake class :', rec_fake)\n",
    "\n",
    "    f1_fake = 2 * prec_fake * rec_fake / ( prec_fake + rec_fake)\n",
    "    print('F1_Fake :', f1_fake)\n",
    "\n",
    "    #######################################################\n",
    "    ###################For Real Class######################\n",
    "    prec_real = tn/(tn + fn)\n",
    "    print('\\nPrecision for Real class :', prec_real)\n",
    "\n",
    "    rec_real = tn/(tn + fp)\n",
    "    print('Recall for Real class :', rec_real)\n",
    "\n",
    "    f1_real = 2 * prec_real * rec_real / ( prec_real + rec_real)\n",
    "    print('F1_Real :', f1_real)\n",
    "\n",
    "    #################################################################\n",
    "    f1_mac = (f1_real + f1_fake )/2\n",
    "    print('\\nF1_Macro :', f1_mac)\n",
    "    print('The Accuracy score is: ', accuracy_score(orig['label'], pred['label']))\n",
    "\n",
    "\n",
    "    #Calculate metrics (f1 ) for each label, \n",
    "    #and find their average weighted by support (the number of true instances for each label).\n",
    "    #This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "    f1_weighted = (250 /400) * f1_real + (150 /400) * f1_fake\n",
    "    print('\\nF1_Average :', f1_weighted)\n",
    "    print(\"-------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c390f69",
   "metadata": {
    "id": "8c390f69"
   },
   "outputs": [],
   "source": [
    "# 6 layers\n",
    "v_stw = 1\n",
    "v_lma = 1\n",
    "v_ngram = 1\n",
    "v_how = \"all\"\n",
    "v_ch_ngram = 0\n",
    "v_ch_how = None #\"exact\"\n",
    "\n",
    "v_char_level=False # word level tokenizer\n",
    "print(\"==============================\")\n",
    "for i in range(1,4):\n",
    "    result_file=\"WordLevel-CNN-\"+str(v_stw)+\"-\"+str(v_lma)+\"-\"+str(v_ngram)+\"-\"+str(v_ch_ngram)+\"ch-\"+str(v_ch_how)+\"-Embedding-100-Conv1D-32-6Layers-iter\"+str(i)+\".csv\"\n",
    "    mkExp()\n",
    "print(\"==============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b39b4",
   "metadata": {
    "id": "db3b39b4"
   },
   "outputs": [],
   "source": [
    "# 6 layers\n",
    "v_stw = 1\n",
    "v_lma = 1\n",
    "v_ngram = 1\n",
    "v_how = \"all\"\n",
    "v_ch_ngram = 0\n",
    "v_ch_how = None #\"exact\"\n",
    "\n",
    "v_char_level=True # word level tokenizer\n",
    "print(\"==============================\")\n",
    "for i in range(1,4):\n",
    "    result_file=\"CharLevel-CNN-\"+str(v_stw)+\"-\"+str(v_lma)+\"-\"+str(v_ngram)+\"-\"+str(v_ch_ngram)+\"ch-\"+str(v_ch_how)+\"-Embedding-100-Conv1D-32-6Layers-iter\"+str(i)+\".csv\"\n",
    "    mkExp()\n",
    "print(\"==============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357ad424",
   "metadata": {
    "id": "357ad424"
   },
   "outputs": [],
   "source": [
    "# 6 layers\n",
    "v_stw = 1\n",
    "v_lma = 1\n",
    "v_ngram = 1\n",
    "v_how = \"all\"\n",
    "v_ch_ngram = 2\n",
    "v_ch_how = \"exact\" #\"exact\"\n",
    "\n",
    "v_char_level=True # word level tokenizer\n",
    "print(\"==============================\")\n",
    "for i in range(1,4):\n",
    "    result_file=\"CharLevel-CNN-\"+str(v_stw)+\"-\"+str(v_lma)+\"-\"+str(v_ngram)+\"-\"+str(v_ch_ngram)+\"ch-\"+str(v_ch_how)+\"-Embedding-100-Conv1D-32-6Layers-iter\"+str(i)+\".csv\"\n",
    "    mkExp()\n",
    "print(\"==============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490da14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 layers\n",
    "v_stw = 1\n",
    "v_lma = 1\n",
    "v_ngram = 1\n",
    "v_how = \"all\"\n",
    "v_ch_ngram = 3\n",
    "v_ch_how = \"exact\" #\"exact\"\n",
    "\n",
    "v_char_level=True # word level tokenizer\n",
    "print(\"==============================\")\n",
    "for i in range(1,4):\n",
    "    result_file=\"CharLevel-CNN-\"+str(v_stw)+\"-\"+str(v_lma)+\"-\"+str(v_ngram)+\"-\"+str(v_ch_ngram)+\"ch-\"+str(v_ch_how)+\"-Embedding-100-Conv1D-32-6Layers-iter\"+str(i)+\".csv\"\n",
    "    mkExp()\n",
    "print(\"==============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51f3534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 layers\n",
    "v_stw = 1\n",
    "v_lma = 1\n",
    "v_ngram = 1\n",
    "v_how = \"all\"\n",
    "v_ch_ngram = 3\n",
    "v_ch_how = \"all\" #\"exact\"\n",
    "\n",
    "v_char_level=True # word level tokenizer\n",
    "print(\"==============================\")\n",
    "for i in range(1,4):\n",
    "    result_file=\"CharLevel-CNN-\"+str(v_stw)+\"-\"+str(v_lma)+\"-\"+str(v_ngram)+\"-\"+str(v_ch_ngram)+\"ch-\"+str(v_ch_how)+\"-Embedding-100-Conv1D-32-6Layers-iter\"+str(i)+\".csv\"\n",
    "    mkExp()\n",
    "print(\"==============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b00e47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71126367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#again word level\n",
    "# not going to work\n",
    "# 6 layers\n",
    "v_stw = 1\n",
    "v_lma = 1\n",
    "v_ngram = 1\n",
    "v_how = \"all\"\n",
    "v_ch_ngram = 2\n",
    "v_ch_how = \"exact\" #\"exact\"\n",
    "\n",
    "v_char_level=False # word level tokenizer\n",
    "print(\"==============================\")\n",
    "for i in range(1,4):\n",
    "    result_file=\"WordLevel-CNN-\"+str(v_stw)+\"-\"+str(v_lma)+\"-\"+str(v_ngram)+\"-\"+str(v_ch_ngram)+\"ch-\"+str(v_ch_how)+\"-Embedding-100-Conv1D-32-6Layers-iter\"+str(i)+\".csv\"\n",
    "    mkExp()\n",
    "print(\"==============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23c2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c28e6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ef2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "UrduFake2021.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
